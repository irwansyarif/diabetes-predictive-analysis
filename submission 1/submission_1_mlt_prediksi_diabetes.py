# -*- coding: utf-8 -*-
"""Submission 1 MLT Prediksi Diabetes

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mJB0ai5eSSo2YC9lGx8pDarjGJVy-hVd

# Import Library
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.io as pio
from sklearn.preprocessing import LabelEncoder,StandardScaler
from sklearn.metrics import accuracy_score,confusion_matrix,f1_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
import warnings
warnings.simplefilter('ignore')

"""# Download Dataset"""

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d iammustafatz/diabetes-prediction-dataset

import zipfile
zip_ref = zipfile.ZipFile('/content/diabetes-prediction-dataset.zip', 'r')
zip_ref.extractall('/content')
zip_ref.close()

"""# DATA PREPROCESSING"""

df = pd.read_csv('/content/diabetes_prediction_dataset.csv')

df.info()

df.duplicated().sum()

df.drop_duplicates(inplace=True)
df.duplicated().sum()
df['smoking_history'].value_counts() # 35816 no info so we should drop columns to avoid inaccureate data

df.drop(columns=['smoking_history'],inplace=True) #preprocessing

df

df.info()

"""# EDA"""

cat_features = ['gender']
num_features = ['age','hypertension','heart_disease','bmi','HbA1c_level','blood_glucose_level','diabetes']

feature = cat_features[0]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
df2 = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df2)
count.plot(kind='bar', title=feature);

df.hist(bins=50, figsize=(20,15))
plt.show()

"""# MULTIVARIATE ANALYSIS"""

sns.pairplot(df, diag_kind = 'kde')

sns.heatmap(df.corr(numeric_only=True),annot=True)
plt.show()

"""# BIVARIATE ANALYSIS"""

sns.boxplot(x ='diabetes', y='bmi', data=df)
plt.show()

sns.boxplot(x='diabetes', y='age', data=df)
plt.show()

sns.countplot(x = 'gender', hue  = 'diabetes', data=df)
plt.show()

sns.boxplot(x ='diabetes', y= 'HbA1c_level', data=df)
plt.show()

sns.boxplot(x = 'diabetes', y= 'blood_glucose_level', data =df)
plt.show()

sns.scatterplot(x='age', y='bmi', hue ='diabetes', data=df)
plt.show()

sns.violinplot(x= 'diabetes', y='bmi', hue="gender",  data=df)
plt.show()

sns.boxplot(x='diabetes', y='bmi', hue='gender', data=df)
plt.show()

sns.boxplot(x= 'diabetes', y='bmi', hue='gender', data=df)
plt.show()

sns.boxplot(x='diabetes', y='age', hue='gender', data=df)
plt.show()

encoder = LabelEncoder()
df['gender'] = encoder.fit_transform(df['gender'])

numeric_columns = df.select_dtypes(include=['number']).columns  # Semua angka
Q1 = df[numeric_columns].quantile(0.25)
Q3 = df[numeric_columns].quantile(0.75)
IQR = Q3 - Q1
df_clean = df[~((df[numeric_columns] < (Q1 - 1.5 * IQR)) | (df[numeric_columns] > (Q3 + 1.5 * IQR))).any(axis=1)]

"""## Split dataset

#### Pembagian dataset menjadi 80% digunakan untuk training model dan 20% untuk mengevaluasi model.
"""

from sklearn.model_selection import train_test_split
# Menentukan fitur (X) dan label (y)
X = df.drop(["diabetes"],axis =1)
y = df["diabetes"]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Print dataset sizes
print("Training set size:", X_train.shape)
print("Testing set size:", X_test.shape)

"""## Normalisasi

#### MinMaxScaler adalah teknik normalisasi yang mengubah nilai fitur atau variabel ke dalam rentang [0,1] yang berarti bahwa nilai minimum dan maksimum dari fitur/variabel masing-masing adalah 0 dan 1
"""

from sklearn.preprocessing import MinMaxScaler

# Normalisasi dengan MinMaxScaler
scaler = MinMaxScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

"""# Make Models"""

# Menyiapkan dataframe untuk analisis model
models = pd.DataFrame(index=['accuracy_score'],
                      columns=['KNN', 'RandomForest', 'SVM', 'Decision Tree','Naive Bayes'])

"""# KNN"""

from sklearn.neighbors import KNeighborsClassifier

# Buat model prediksi dengan KNN
model_knn = KNeighborsClassifier(n_neighbors=3)
model_knn.fit(X_train, y_train)

# Lakukan prediksi dengan model KNN
knn_pred = model_knn.predict(X_test)

# Hitung metriks akurasi dan simpan hasilnya
models.loc['accuracy_score','KNN'] = accuracy_score(y_test, knn_pred)

"""# Random Forest"""

from sklearn.ensemble import RandomForestClassifier

# Train a model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

from sklearn.metrics import confusion_matrix, classification_report

import matplotlib.pyplot as plt


# Lakukan prediksi dengan model Random Forest
rf_pred = rf_model.predict(X_test)

# Hitung metriks akurasi dan simpan hasilnya
models.loc['accuracy_score','RandomForest'] = accuracy_score(y_test, rf_pred)

import seaborn as sns

# Create and visualize confusion matrix
cm = confusion_matrix(y_test, rf_pred)

# Plot confusion matrix using seaborn
plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive', 'Neutral'], yticklabels=['Negative', 'Positive', 'Neutral'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""# SVM"""

from sklearn.svm import SVC

# Train an SVM model
svm_model = SVC(kernel='linear', random_state=42, C=7.5)
svm_model.fit(X_train, y_train)

# Lakukan prediksi dengan model SVM Classifier
svm_pred = svm_model.predict(X_test)

# Hitung metriks akurasi dan simpan hasilnya
models.loc['accuracy_score','SVM'] = accuracy_score(y_test, svm_pred)

"""# Decision Tree"""

from sklearn.tree import DecisionTreeClassifier

# Train a Decision Tree model
dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train, y_train)

# Lakukan prediksi dengan model Decision Tree Model
dt_pred = dt_model.predict(X_test)

# Hitung metriks akurasi dan simpan hasilnya
models.loc['accuracy_score','Decision Tree'] = accuracy_score(y_test, dt_pred)

"""# Naive Bayes"""

from sklearn.naive_bayes import BernoulliNB

# Buat model prediksi dengan Bernoulli Naive Bayes
model_nb = BernoulliNB()
model_nb.fit(X_train, y_train)

# Lakukan prediksi dengan model Naive Bayes
nb_pred = model_nb.predict(X_test)

# Hitung metriks akurasi dan simpan hasilnya
models.loc['accuracy_score','Naive Bayes'] = accuracy_score(y_test, nb_pred)

"""# Evaluasi model"""

y_pred_knn = model_knn.predict(X_test)
y_pred_svm = svm_model.predict(X_test)
y_pred_rf = rf_model.predict(X_test)
y_pred_dt = dt_model.predict(X_test)
y_pred_nb = model_nb.predict(X_test)

# Calculate accuracy
accuracy_scores = {
    "KNN": accuracy_score(y_test, knn_pred),
    "SVM": accuracy_score(y_test, y_pred_svm),
    "Random Forest": accuracy_score(y_test, y_pred_rf),
    "Decision Tree": accuracy_score(y_test, y_pred_dt),
    "Naive Bayes": accuracy_score(y_test, nb_pred)
}

plt.figure(figsize=(8, 5))
bars = plt.bar(accuracy_scores.keys(), accuracy_scores.values(), color=['blue','orange' ,'green', 'red','purple'])
;
# Add accuracy values on top of bars
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval, f"{yval:.4f}", ha='center', va='bottom', fontsize=12)

plt.xlabel("Model")
plt.ylabel("Accuracy Score")
plt.ylim(0, 1)  # Accuracy is between 0 and 1
plt.title("Model Performance Comparison")
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Save the plot as an image
plt.savefig("model_accuracy_comparison.png", dpi=300, bbox_inches='tight')

# Show the plot
plt.show()

"""##### Dari diagram diatas dapat kita lihat bahwa model Random Forest adalah Model tertinggi daripada keempat model lainnya. Dengan itu, maka model ini yang akan dipakai.

## Penutup

#### Saat ini model untuk memprediksi diabetes telah didapatkan. Dengan model ini diimplementasi lebih lanjut untuk dijadikan sebuah aplikasi yang siap digunakan. Namun, model ini juga masih dapat dikembangkan dengan mencoba algoritma lain, menambahkan fine-tuning, atau merubah dataset.
"""